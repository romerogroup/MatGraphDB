{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.nn import CGConv, global_add_pool, global_mean_pool, global_max_pool, Sequential\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torchmetrics.functional import mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "from poly_graphs_lib.poly_dataset import PolyhedraDataset\n",
    "from poly_graphs_lib.callbacks import EarlyStopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolyhedronModel(nn.Module):\n",
    "    \"\"\"This is the main Polyhedron Model. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        n_node_features : int\n",
    "        The number of node features\n",
    "    n_edge_features : int\n",
    "        The number of edge features, by default 2\n",
    "    n_gc_layers : int, optional\n",
    "        The number of graph convolution layers, by default 1\n",
    "    global_pooling_method : str, optional\n",
    "        The global pooling method to be used, by default 'add'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                n_node_features:int,\n",
    "                n_edge_features:int, \n",
    "                n_gc_layers:int=1, \n",
    "                n_hidden_layers:List[int]=[5],\n",
    "                global_pooling_method:str='add'):\n",
    "        \"\"\"This is the main Polyhedron Model. \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "         n_node_features : int\n",
    "            The number of node features\n",
    "        n_edge_features : int\n",
    "            The number of edge features, by default 2\n",
    "        n_gc_layers : int, optional\n",
    "            The number of graph convolution layers, by default 1\n",
    "        global_pooling_method : str, optional\n",
    "            The global pooling method to be used, by default 'add'\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "            \n",
    "        layers=[]\n",
    "        for i_gc_layer in range(n_gc_layers):\n",
    "            if i_gc_layer == 0:\n",
    "                vals = \" x, edge_index, edge_attr -> x0 \"\n",
    "            else:\n",
    "                vals = \" x\" + repr(i_gc_layer - 1) + \" , edge_index, edge_attr -> x\" + repr(i_gc_layer)\n",
    "\n",
    "            layers.append((pyg_nn.CGConv(n_node_features, dim=n_edge_features),vals))\n",
    "\n",
    "        # self.cg_conv_layers = Sequential(\" x, edge_index, edge_attr, batch \" , layers)\n",
    "        self.bn = pyg_nn.norm.BatchNorm(in_channels=n_node_features)\n",
    "\n",
    "        self.cg_conv_layers = Sequential(\" x, edge_index, edge_attr \" , layers)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.linear_1 = nn.Linear( n_node_features, n_hidden_layers[0])\n",
    "        self.out_layer= nn.Linear( n_hidden_layers[-1],  1)\n",
    "        \n",
    "        # self.layer_norm = nn.LayerNorm(self.angle_fea_len)\n",
    "\n",
    "        if global_pooling_method == 'add':\n",
    "            self.global_pooling_layer = global_add_pool\n",
    "        elif global_pooling_method == 'mean':\n",
    "            self.global_pooling_layer = global_mean_pool\n",
    "        elif global_pooling_method == 'max':\n",
    "            self.global_pooling_layer = global_max_pool\n",
    "\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        \"\"\"The forward pass of of the network\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : pygeometic.Data\n",
    "            The pygeometric data object\n",
    "        targets : float, optional\n",
    "            The target value to use to calculate the loss, by default None\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        _type_\n",
    "            _description_\n",
    "        \"\"\"\n",
    "\n",
    "        out = self.bn(x)\n",
    "        print(x.shape)\n",
    "        # Convolutional layers combine nodes and edge interactions\n",
    "        out = self.cg_conv_layers(x.x, x.edge_index, x.edge_attr ) # out -> (n_total_node_in_batch, n_node_features)\n",
    "        out = self.sig(out) # out -> (n_total_nodes_in_batch, n_node_features)\n",
    "\n",
    "        # Fully connected layer\n",
    "        out = self.linear_1(out) # out -> (n_total_nodes_in_batch, n_hidden_layers[0])\n",
    "        out = self.sig(out) # out -> (n_total_nodes_in_batch, n_hidden_layers[0])\n",
    "\n",
    "        # batch is index list differteriating which nodes belong to which graph\n",
    "        out = self.global_pooling_layer(out, batch = x.batch) # out -> (n_graphs, n_hidden_layers[0])\n",
    "\n",
    "        out = self.out_layer(out) # out -> (n_graphs, 1)\n",
    "        out = self.relu(out) # out -> (n_graphs, 1)\n",
    "\n",
    "        # Loss handling\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "            mape_loss = None\n",
    "        else:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "            mape_loss = mean_absolute_percentage_error(torch.squeeze(out, dim=1), targets)\n",
    "            loss = loss_fn(torch.squeeze(out, dim=1), targets)\n",
    "\n",
    "        return out,  loss, mape_loss\n",
    "\n",
    "    def generate_encoding(self, x):\n",
    "        \"\"\"This method generates the polyhedra encoding\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : pyg.Data object\n",
    "            The pygeometric Data object\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The encoded polyhedra vector\n",
    "        \"\"\"\n",
    "\n",
    "        out = self.cg_conv_layers(x.x, x.edge_index, x.edge_attr )\n",
    "        out = self.sig(out)\n",
    "        out = self.linear_1(out) # out -> (n_total_atoms_in_batch, 1)\n",
    "        out = self.sig(out)\n",
    "        out = self.global_pooling_layer(out, batch = x.batch)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lllang\\Desktop\\Romero Group Research\\Research Projects\\crystal_generation_project\\Graph_Network_Project\n"
     ]
    }
   ],
   "source": [
    "project_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# hyperparameters\n",
    "save_model = True\n",
    "\n",
    "# Training params\n",
    "n_epochs = 1000\n",
    "learning_rate = 1e-2\n",
    "batch_size = 8\n",
    "early_stopping_patience = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# polyhedron model parameters\n",
    "n_gc_layers = 2\n",
    "global_pooling_method = 'add'\n",
    "\n",
    "# dataset parameters\n",
    "dataset = 'material_random_polyhedra'\n",
    "feasture_set_index = 3\n",
    "y_val = ['energy_per_verts','dihedral_energy'][0]\n",
    "\n",
    "###################################################################\n",
    "# Start of the the training run\n",
    "###################################################################\n",
    "\n",
    "train_dir = f\"{project_dir}{os.sep}data{os.sep}{dataset}{os.sep}feature_set_{feasture_set_index}{os.sep}train\"\n",
    "test_dir = f\"{project_dir}{os.sep}data{os.sep}{dataset}{os.sep}feature_set_{feasture_set_index}{os.sep}test\"\n",
    "val_dir = f\"{project_dir}{os.sep}data{os.sep}{dataset}{os.sep}feature_set_{feasture_set_index}{os.sep}val\"\n",
    "\n",
    "val_dataset = PolyhedraDataset(database_dir=val_dir, device=device, y_val=y_val)\n",
    "n_node_features = val_dataset[0].x.shape[1]\n",
    "n_edge_features = val_dataset[0].edge_attr.shape[1]\n",
    "del val_dataset\n",
    "\n",
    "\n",
    "train_dataset = PolyhedraDataset(database_dir=train_dir,device=device, y_val=y_val)\n",
    "# train_2_dataset = PolyhedraDataset(database_dir=train_2_dir,device=device, y_val=y_val)\n",
    "test_dataset = PolyhedraDataset(database_dir=test_dir,device=device, y_val=y_val)\n",
    "val_dataset = PolyhedraDataset(database_dir=val_dir,device=device, y_val=y_val)\n",
    "\n",
    "n_train = len(train_dataset)\n",
    "n_validation = len(val_dataset)\n",
    "\n",
    "# Creating data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=0)\n",
    "# train_2_loader = DataLoader(train_2_dataset, batch_size=batch_size, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolyhedronModel(n_node_features=n_node_features, \n",
    "                                n_edge_features=n_edge_features, \n",
    "                                n_gc_layers=n_gc_layers,\n",
    "                                global_pooling_method=global_pooling_method)\n",
    "m = model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "es = EarlyStopping(patience = early_stopping_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(x=[378, 9], edge_index=[2, 567], edge_attr=[567, 1], y=[8], pos=[8], batch=[378], ptr=[9])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mz:\\Anaconda\\envs\\voronoi\\lib\\site-packages\\torch_geometric\\data\\storage.py:62\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[key]\n\u001b[0;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mz:\\Anaconda\\envs\\voronoi\\lib\\site-packages\\torch_geometric\\data\\storage.py:85\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m---> 85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'dim'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(test_loader))\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(sample)\n\u001b[1;32m----> 3\u001b[0m model(sample , targets \u001b[39m=\u001b[39;49m sample\u001b[39m.\u001b[39;49my)\n",
      "File \u001b[1;32mz:\\Anaconda\\envs\\voronoi\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [15], line 85\u001b[0m, in \u001b[0;36mPolyhedronModel.forward\u001b[1;34m(self, x, targets)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, targets\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     70\u001b[0m     \u001b[39m\"\"\"The forward pass of of the network\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39m        _description_\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn(x\u001b[39m=\u001b[39;49mx)\n\u001b[0;32m     86\u001b[0m     \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     87\u001b[0m     \u001b[39m# Convolutional layers combine nodes and edge interactions\u001b[39;00m\n",
      "File \u001b[1;32mz:\\Anaconda\\envs\\voronoi\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mz:\\Anaconda\\envs\\voronoi\\lib\\site-packages\\torch_geometric\\nn\\norm\\batch_norm.py:69\u001b[0m, in \u001b[0;36mBatchNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mallow_single_element \u001b[39mand\u001b[39;00m x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mbatch_norm(\n\u001b[0;32m     60\u001b[0m         x,\n\u001b[0;32m     61\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mrunning_mean,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39meps,\n\u001b[0;32m     68\u001b[0m     )\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(x)\n",
      "File \u001b[1;32mz:\\Anaconda\\envs\\voronoi\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mz:\\Anaconda\\envs\\voronoi\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_input_dim(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[0;32m    141\u001b[0m     \u001b[39m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[39m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmomentum \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mz:\\Anaconda\\envs\\voronoi\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:300\u001b[0m, in \u001b[0;36mBatchNorm1d._check_input_dim\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_input_dim\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m--> 300\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mdim() \u001b[39m!=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim() \u001b[39m!=\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[0;32m    301\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    302\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mexpected 2D or 3D input (got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39mD input)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim())\n\u001b[0;32m    303\u001b[0m         )\n",
      "File \u001b[1;32mz:\\Anaconda\\envs\\voronoi\\lib\\site-packages\\torch_geometric\\data\\data.py:428\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_store\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[0;32m    423\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    424\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m object was created by an older version of PyG. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this error occurred while loading an already existing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdataset, remove the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mprocessed/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m directory in the dataset\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mroot folder and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 428\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store, key)\n",
      "File \u001b[1;32mz:\\Anaconda\\envs\\voronoi\\lib\\site-packages\\torch_geometric\\data\\storage.py:64\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[0;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "sample = next(iter(test_loader))\n",
    "print(sample)\n",
    "model(sample , targets = sample.y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch_0 = 0\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    n_epoch = n_epoch_0 + epoch\n",
    "    batch_train_loss = 0.0\n",
    "    batch_train_mape = 0.0\n",
    "    for i,sample in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        out, train_loss, mape_loss = model(sample , targets = sample.y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_train_loss += train_loss.item()\n",
    "        batch_train_mape += mape_loss.item()\n",
    "\n",
    "    batch_train_loss = batch_train_loss / (i+1)\n",
    "    batch_train_mape = batch_train_mape / (i+1)\n",
    "\n",
    "    batch_val_loss = 0.0\n",
    "    batch_val_mape = 0.0\n",
    "    for i,sample in enumerate(val_loader):\n",
    "        torch.set_grad_enabled(False)\n",
    "        out, val_loss, mape_val_loss = model(sample , targets = sample.y)\n",
    "        torch.set_grad_enabled(True)\n",
    "        batch_val_loss += val_loss.item()\n",
    "        batch_val_mape += mape_val_loss.item()\n",
    "    batch_val_loss = batch_val_loss / (i+1)\n",
    "    batch_val_mape = batch_val_mape / (i+1)\n",
    "\n",
    "\n",
    "    batch_test_loss = 0.0\n",
    "    batch_test_mape = 0.0\n",
    "    for i,sample in enumerate(test_loader):\n",
    "        torch.set_grad_enabled(False)\n",
    "        out, test_loss, mape_test_loss = model(sample , targets = sample.y)\n",
    "        torch.set_grad_enabled(True)\n",
    "        batch_test_loss += test_loss.item()\n",
    "        batch_test_mape += mape_test_loss.item()\n",
    "    batch_test_loss = batch_test_loss / (i+1)\n",
    "    batch_test_mape = batch_test_mape / (i+1)\n",
    "\n",
    "\n",
    "    # val_loss *= (factor)  # to put it on the same scale as the training running loss)\n",
    "    if n_epoch % 10 == 0:\n",
    "        print(repr(n_epoch) + \",  \" + repr(batch_train_loss) + \",  \" + repr(batch_val_loss)+ \",  \" + repr(batch_test_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "voronoi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc33c7ce9a2731e93e9a126b5b58eb75486c3d9aa5609e749b24982b0e766b04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
